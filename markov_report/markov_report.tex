\documentclass{article}
\usepackage{amsmath, amsfonts, mathtools}
\usepackage{algorithm, algpseudocode}
\usepackage{geometry}
\geometry{
    a4paper,
    total={170mm,257mm},
    left=20mm,
    top=20mm,
}
\usepackage{indentfirst}
\title{title}
\author{tinyynoob}
\date{\today}

\begin{document}
\section{Introduction}

In this report, we want to introduce a method to find the most-probable history of a Markov chain, 
that is, to find the path which has highest probability.

First, we discuss the problem in a non-hidden Markov chain. 
And later, the same problem in a hidden Markov chain will be covered. 
The solution is well-known as {\it the Viterbi algorithm}.

To make the problem simple, we discuss in a Markov chain which has only finite number of states.


\section{Non-Hidden Cases} \setlength{\parindent}{0cm}

Let $(\Omega,\mathcal{F},\mathcal{P})$ be a probability space. \\
Suppose that we have a Markov chain $(X_n)_{n\geq 0}$ with finite state-space and the transition matrix $P$. \\
Since the state space is finite, there must be an one-to-one correspondence between the state space and $I=\{1,2,\dotsc,N\}$. \\
Thus it is reasonable to consider $X_n:\Omega\to I$ for all $n\in\mathbb{N}_0$, and $P=p[i,j]_{i,j\in I}$. \\
\\
Here are some notations we use:
\begin{itemize}
    \item $\mathbb{N}$ denotes the set $\{1,2,3,\dotsc\}$
    \item $\mathbb{N}_0$ denotes the set $\{0\}\cup\mathbb{N}$
    \item $x^{(n)}=(x_0,x_1,\dotsc,x^{n})\in I^{n+1}$ is an apparent variable denoting a state sequence of length $n$ for some $n\in\mathbb{N}_0$
    \item $\mathbf{X}^{(n)}=(X_0,X_1,\dotsc,X_n)$ denotes a Markov history from time $0$ to some $n\in\mathbb{N}_0$
\end{itemize}
$ $\\
Our problem is described as follow: \\
Given an initial distribution $\lambda=(\lambda_1,\lambda_2,\dotsc,\lambda_N)$ and a termination time $T\in\mathbb{N}$, 
what is the deterministic state sequence $x^*=(x^*_0,x^*_1,x^*_2,\dotsc,x^*_T)$ satisfying
\begin{align}\label{eq:1}
    \mathcal{P}(\mathbf{X}^{(T)}=x^*) = \max \left\{\mathcal{P}(\mathbf{X}^{(T)}=x^{(T)})\mid x^{(T)}\in I^{T+1}\right\}
\end{align}

Assume that such $x^*$ is unique, the representation of equation \eqref{eq:1} is equivalent to
\begin{align}\label{eq:2}
    x^* = \arg\max_{x^{(T)}}\left\{\mathcal{P}(\mathbf{X}^{(T)}=x^{(T)})\right\}
\end{align}
$ $\\
Now we want to show that the sequence $x^*$ can be found recursively. \\
First, we know the (a priori) probability of a deterministic path can be computed by
\begin{align*}
    \mathcal{P}(\mathbf{X}^{(n)}=x^{(n)}) &= \mathcal{P}(X_0=x_0,X_1=x_1,\dotsc,X_n=x_n) \\
    &= \mathcal{P}(X_0=x_0) \mathcal{P}(X_1=x_1\mid X_0=x_0) \cdots \mathcal{P}(X_n=x_n\mid X_0=x_0,\dotsc,X_{n-1}=x_{n-1}) \\
    &= \mathcal{P}(X_0=x_0)\mathcal{P}(X_1=x_1\mid X_0=x_0)\cdots\mathcal{P}(X_n=x_n\mid X_{n-1}=x_{n-1})\quad \text{(by Markov assumption)} \\
    &= \mathcal{P}(X_0=x_0)\,p[x_0,x_1]\,\cdots \,p[x_{n-1},x_n]
\end{align*}

Before we move forward, we need a convenient notation. \\
For $n\in\mathbb{N}_0$ and $i\in I$, define $y(n,i)$ and $a(n,i)$.
\begin{itemize}
    \item $y(n,i)=(y_0,y_1,\dotsc,y_{n-1},i)\in I^{n+1}$ denotes the state sequence satisfies
    $$\mathcal{P}(\mathbf{X}^{(n)}=y(n,i)) = \max\left\{\mathcal{P}(\mathbf{X}^{(n-1)}=x^{(n-1)},X_n=i)\mid x^{(n-1)}\in I^{n} \right\}$$
    \item $a(n,i)$ denotes the probability $\max\left\{\mathcal{P}(\mathbf{X}^{(n-1)}=x^{(n-1)},X_n=i)\mid x^{(n-1)}\in I^{n} \right\}$
\end{itemize}
Given $n\in\mathbb{N}_0$ and $i\in I$, since the state-space is finite, $y(n,i)$ is guarantee to exist. \\
Though $y(n,i)$ may not be unique, all of them would have the same probability, that is, $a(n,i)$ is unique.
Therefore, we can assign any sequence $x^{(n)}\in I^{n+1}$ with $x_n=i$ satisfying $\mathcal{P}(\mathbf{X}^{(n)}=x^{(n)})=a(n,i)$ as $y(n,i)$. \\
\\
Based on the knowing of $y(n,j)\;\forall\,j\in I$, we want to obtain $y(n+1,i)$. \\
We have
\begin{align}\label{eq:3}
    \mathcal{P}(X_0=x_0,X_1=x_1,\dotsc,X_{n+1}=x_{n+1}) &= \mathcal{P}(X_0=x_0,X_1=x_1,\dotsc,X_n=x_n)\,p[x_n,x_{n+1}]
\end{align}

For $i\in I$, by definition, $y(n+1,i)$ satisfies
\begin{align}
    \mathcal{P}(y(n+1, i)) &= \max\left\{\mathcal{P}(\mathbf{X}^{(n+1)}=x^{(n+1)})\mid x^{(n+1)}=(x_0,x_1,\dotsc,x_n,i)\right\}
\end{align}

\begin{align}
    \mathcal{P}(y(n+1, i)) &= \max\left\{\mathcal{P}(\mathbf{X}^{(n)}=x^{(n)}, X_{n+1}=i)\mid x^{(n+1)}\in I^{n+1}\right\} \nonumber\\
    &= \max \left\{\mathcal{P}(\mathbf{X}^{(n-1)}=x^{(n-1)},X_n=j,X_{n+1}=i)\mid x^{(n-1)}\in I^{n}, j\in I \right\} \quad\text{ by \eqref{eq:3}}\nonumber\\
    &= \max \left\{\mathcal{P}(\mathbf{X}^{(n-1)}=x^{(n-1)},X_n=j)\mathcal{P}(X_{n+1}=i\mid X_n=j) \mid x^{(n-1)}\in I^{n}, j\in I \right\} \quad\text{(by Markov assumption)} \nonumber\\
    &= \max \left\{\mathcal{P}(\mathbf{X}^{(n-1)}=x^{(n-1)},X_n=j)\, p[j,i] \mid x^{(n-1)}\in I^{n}, j\in I \right\} \label{eq:4}
\end{align}
Since we already know the sequence to maximize $\mathcal{P}(\mathbf{X}^{(n-1)}=x^{(n-1)},X_n=j)$ for each $j\in I$, 
we do not need to go through all $I^{n+1}$ possible cases. We can search among the cases conditioning on the last state $n$. Hence,
\begin{align}
    \mathcal{P}(y(n+1, i)) &= \eqref{eq:4} \nonumber\\
    &= \max\left\{\mathcal{P}(\mathbf{X}^{(n)}=y(n,j))\,p[j,i] \mid j\in I \right\}
\end{align}
$ $\\
Now, we need a starting point to complete the recursive algorithm. \\
Since we have the initial distribution of the chain, 
the algorithm is complete, as follow:
\begin{algorithm}[H]
    \text{\bf Initialization:}
    \begin{algorithmic}
        \For{$i\in I$}
            \State $a(0,i)\gets \lambda_i$
            \State $y(0,i)\gets i$
        \EndFor
    \end{algorithmic}
    \text{\bf Recursion:}
    \begin{algorithmic}
        \For{$n=1$ to $n=T$}
            \For{$i\in I$}
                \State $\displaystyle a(n,i)\gets \max_{j}\left\{a(n-1,j)\,p[j,i]\mid j\in I \right\}$
                \State $\displaystyle k\gets \arg\max_{j}\left\{a(n-1,j)\,p[j,i]\mid j\in I \right\}$
                \State $y(n,i)\gets (y(n-1,k),i)$
            \EndFor
        \EndFor
    \end{algorithmic}
    \text{\bf Termination:}
    \begin{algorithmic}
        \State $\displaystyle x^*\gets \arg\max_{y(T,i)}\left\{\mathcal{P}(\mathbf{X}^{(T)}=y(T,i)) \mid i\in I\right\}$
    \end{algorithmic}
\end{algorithm}

\section{Hidden Cases}
In a hidden Markov chain, we can never know the state sequence even when the chain has terminated. \\
Hence there is a small difference between the problem we are discussing in these two situations.
\begin{itemize}
    \item In the non-hidden cases, we try to predict the most probable result before we would see it.
    \item In hidden cases, we try to figure out what state sequence was gone through after we have seen some outcome.
\end{itemize}
Most of the variables remain the same as before. Here, we have additional information, a sequence of observations $O=(o_1,o_2,\dotsc,o_T)$, 
in which each element comes from the observation space $Q=\{1,2,3,\dotsc,V\}$ where $V\in\mathbb{N}$. \\
Though the state sequence is hidden, we have an observation likelihood (probability) functions $b$ such that for $1\leq n\leq T$ and $i\in I$, $b(n,i)=P(o_n\mid X_n=i)$. \\
\\
Then our problem is described as follow:\\
Given an initial distribution $\lambda=(\lambda_1,\lambda_2,\dotsc,\lambda_N)$, the termination time $T\in \mathbb{N}$, 
and a sequence of oberservations $O=(o_1,o_2,\dotsc,o_T)$, what is the deterministic sequence $x^*=(x^*_0,x^*_1,x^*_2,\dotsc,x^*_T)$ satisfying
\begin{align}\label{eq:5}
    \mathcal{P}(\mathbf{X}^{(T)}=x^*) = \max \left\{\left.\mathcal{P}\left(\mathbf{X}^{(T)}=x^{(T)}, O\right)\right| x^{(T)}\in I^{T+1}\right\}
\end{align}
And there are two assumptions:
\begin{itemize}
    \item $o_1,o_2,\dotsc,o_T$ are independent
    \item At any time step, the observation depends only on the state at that time
\end{itemize}
$ $\\
Similarly as in the previous section, for $1\leq n\leq T$, define $O^{(n)},y(n,i),\alpha(n,i)$.
\begin{itemize}
    \item $O^{(n)}=(o_1,o_2,\dotsc,o_n)$ denotes the observation sequence to time $n$
    \item $y(n,i)=(y_0,y_1,\dotsc,y_{n-1},i)\in I^{n+1}$ denotes the state sequence satisfying
    \begin{align}
        \mathcal{P}\left(\mathbf{X}^{(n)}=y(n,i)\mid O^{(n)}\right) = \max\left\{\left.\mathcal{P}\left(\mathbf{X}^{(n-1)}=x^{(n-1)}, X_n=i\mid O^{(n)}\right)\right| x^{(n-1)}\in I^{n}\right\} \label{eq:6}
    \end{align}
    \item $\alpha(n,i) = \max\left\{\left.\mathcal{P}\left(\mathbf{X}^{(n-1)}=x^{(n-1)}, X_n=i, O^{(n)}\right)\right| x^{(n-1)}\in I^{n}\right\}$
\end{itemize}
$ $\\
Since
\begin{align}
    \mathcal{P}\left(\mathbf{X}^{(n-1)}=x^{(n-1)},X_n=i\mid O^{(n)}\right) &= \frac{\mathcal{P}\left(\mathbf{X}^{(n-1)}=x^{(n-1)},X_n=i, O^{(n)}\right)}{\mathcal{P}(O^{(n)})}
\end{align}
then
\begin{multline}\label{eq:9}
    \arg\max\left\{\left.\mathcal{P}\left(\mathbf{X}^{(n-1)}=x^{(n-1)}, X_n=i\mid O^{(n)}\right)\right| x^{(n-1)}\in I^{n}\right\} \\
    =\arg\max\left\{\left.\mathcal{P}\left(\mathbf{X}^{(n-1)}=x^{(n-1)}, X_n=i, O^{(n)}\right)\right| x^{(n-1)}\in I^{n}\right\} 
\end{multline}
Suppose that we know $y(n,j)\;\forall\,j\in I$. \\
We want to obtain $y(n+1,i)=(y_0,y_1,\dotsc,y_n,i)$. According to equation \eqref{eq:9},
\begin{align}
    (y_0,y_1,\dotsc,y_n) &= \arg\max_{x^{(n)}}\left\{\left.\mathcal{P}\left(\mathbf{X}^{(n)}=x^{(n)}, X_{n+1}=i, O^{(n+1)}\right)\right| x^{(n)}\in I^{n+1}\right\} \nonumber\\
    &= \arg\max_{(x^{(n-1)},j)}\left\{\left.\mathcal{P}\left(\mathbf{X}^{(n-1)}=x^{(n-1)}, X_n=j, X_{n+1}=i, O^{(n+1)}\right)\right| x^{(n-1)}\in I^{n}, j\in I\right\} \nonumber\\
    &= \arg\max\left\{\left.\mathcal{P}\left(\mathbf{X}^{(n-1)}=x^{(n-1)}, X_n=j, X_{n+1}=i, O^{(n)},o_{n+1}\right)\right| x^{(n-1)}\in I^{n}, j\in I\right\} \label{eq:8}
\end{align}
Since $o_{n+1}$ is independent with $O^{(n)}$ and depends only on $X_{n+1}$,
\begin{align}
    \eqref{eq:8} &= \arg\max\left\{\left.\mathcal{P}\left(\mathbf{X}^{(n-1)}=x^{(n-1)}, X_n=j, X_{n+1}=i, O^{(n)}\right)\mathcal{P}\left(o_{n+1}\mid X_{n+1}=i\right)\right| x^{(n-1)}\in I^{n}, j\in I\right\} \\
    &= \arg\max\left\{\left.\mathcal{P}\left(\mathbf{X}^{(n-1)}=x^{(n-1)}, X_n=j, X_{n+1}=i, O^{(n)}\right)\,b(n+1,i)\right| x^{(n-1)}\in I^{n}, j\in I\right\} \\
    &= \arg\max\left\{\left.\mathcal{P}\left(\mathbf{X}^{(n-1)}=x^{(n-1)}, X_n=j, O^{(n)}\right)\,p[j,i]\,b(n+1,i) \right| x^{(n-1)}\in I^n, j\in I\right\} \quad\text{(by Markov assumption)} \label{eq:10}
\end{align}
Similar to what we have done in \eqref{eq:4}, we have the result
\begin{align}
    (y_0,y_1,\dotsc,y_n) &= \eqref{eq:10} \\
    &= \left(y(n-1,j), \:\arg\max_{j}\left\{\left.\mathcal{P}\left(\mathbf{X}^{(n)}=y(n,j)\right)\,p[j.i]\,b(n+1,i) \right| j\in I \right\}\right)
\end{align}
Hence in the hidden case, we can solve the problem recursively as well. \\
\\
Here is the pseudocode of the algorithm:
\begin{algorithm}[H]
    \text{\bf Initialization:}
    \begin{algorithmic}
        \For{$i\in I$}
            \State $\alpha(0,i)\gets \lambda_i$
            \State $y(0,i)\gets i$
        \EndFor
    \end{algorithmic}
    \text{\bf Recursion:}
    \begin{algorithmic}
        \For{$n=1$ to $n=T$}
            \For{$i\in I$}
                \State $\displaystyle \alpha(n,i)\gets \max_{j}\left\{\alpha(n-1,j)\,b(n,i)\,p[j,i]\mid j\in I \right\}$
                \State $\displaystyle k\gets \arg\max_{j}\left\{\alpha(n-1,j)\,b(n,i)\,p[j,i]\mid j\in I \right\}$
                \State $y(n,i)\gets (y(n-1,k),i)$
            \EndFor
        \EndFor
    \end{algorithmic}
    \text{\bf Termination:}
    \begin{algorithmic}
        \State $\displaystyle x^*\gets \arg\max_{y(T,i)}\left\{\mathcal{P}\left(\mathbf{X}^{(T)}=y(T,i)\right)\,b(T,i) \mid i\in I\right\}$
    \end{algorithmic}
\end{algorithm}


\end{document}
